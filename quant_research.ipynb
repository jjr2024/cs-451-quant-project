{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Machine Learning to Stock Price Prediction\n",
    "#### Donovan Wood, James Ohr, Andre Xiao  \n",
    "## Abstract\n",
    "* What problem did we address?\n",
    "* What approach(es) did we use to address it?\n",
    "* What are the big-picture results?\n",
    "## Introduction\n",
    "* Prompt: \"Your introduction should describe the big-picture problem that you aimed to address in your project. Whatâ€™s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target.\"\n",
    "\n",
    "In this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question \"What is a company worth?\" Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it's the problem for all quant trading: making a profitable model. \n",
    "\n",
    "An enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Bhandari et al. (2022) and Zhang (2022).\n",
    "\n",
    "@gunduzEfficientPrediction2021 applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\n",
    "\n",
    "@bhandariLSTM2022 apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\n",
    "\n",
    "@zhangPrediction2022 finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang's \"other models\" are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\n",
    "\n",
    "We take some of the \"best practices\" we observe in the above papers, specifically benchmarking with last value and calculating accuracy with R, RMSE, and MAPE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values\n",
    "\n",
    "The potential users are anyone interested in making profitable trades in the stock market. They are the individuals most likely to directly benefit from our work. Nonusers who could be affected by our work are those engaged in the stock market. The obvious affected nonusers are those on the opposite side of each trade as a user. In every trade, there's a buyer and a seller, so in every trade, there's a winner and a loser. These opposing nonusers are the individuals who are most likely to be harmed by the success of our program. \n",
    "\n",
    "Ultimately, the point of the back and forth of markets is price discovery: to help society find the right prices of different companies. This leads to another nonuser effect: with better price discovery and more efficient markets, companies will raise money at prices that are closer to some \"true\" value, which is loosely defined as a value that best reflects the fundamental valuation of the company. Our model does not attempt to predict a true fundamental value for a company, but by making accurate predictions for the next day's price, it should accelerate the market's convergence to an appropriate value.\n",
    "\n",
    "A useful financial trading model should lead to a net societal benefit because better financial markets mean more or less money going to companies and therefore projects, leading to something closer to an \"optimal\" allocation of money in society.\n",
    "\n",
    "We are personally motivated to work on this project because of personal interest, professional relevance, and the difficulty of the problem. All three of us personally invest in the stock market. Two of us (Donovan & James) are double majors in economics and have had experience working in the financial services industry. Andre is interested in pursuing a master's in financial engineering after Middlebury. The problem itself is also inherently challenging: financial markets are constantly adapting and changing, making the findings of previous literature increasingly likely over time to be less applicable to today's markets. This forces us to adopt new techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materials and Methods \n",
    "\n",
    "'XOM', 'CVX', 'COP', 'EPD', 'PXD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX'\n",
    "\n",
    "## Your Data \n",
    "\n",
    "Our data was sourced from Yahoo Finance. We used the `yfinance` library to download historical stock price data for our 10 different stocks. We chose to focus on US-based oil companies. These companies are Exxon Mobil (XOM), Chevron (CVX), ConocoPhillips (COP), Enterprise Products Partners (EPD), Pioneer Natural Resources (PXD), EOG Resources (EOG), Duke Energy (DUK), Marathon Petroleum (MPC), Schlumberger (SLB), and Phillips 66 (PSX). We downloaded the data from May 7th, 2014 to May 7th, 2024. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Your Approach\n",
    "\n",
    "# What features of your data you used as predictors for your models, and what features (if any) you used as targets.\n",
    "\n",
    "We used `SMA_20`, `SMA_50`, `Std_Dev`, `Z_Score`, `RSI`, `Close`, `TTM_P/E` as predictors for our models. \n",
    "\n",
    "The `SMA_20` and `SMA_50` are the 20-day and 50-day simple moving averages of the stock price. The `Std_Dev` is the standard deviation of the stock price. The `Z_Score` is the z-score of the stock price. The `RSI` is the relative strength index of the stock price. The `Close` is the closing price of the stock. The `TTM_P/E` is the trailing twelve months price-to-earnings ratio of the stock.\n",
    "\n",
    "We used the `Close` price as the target variable for our model.\n",
    "\n",
    "# Whether you subset your data in any way, and for what reasons.\n",
    "\n",
    "We used a standard scaler for scaling our data in order to ensure that the data was normalized. We also used a train-test split of 90-10 in order to train our model on 90% of the data and test it on the remaining 10%. We used a 10-year window to predict stock price movements from May 7th, 2024. We used the closing price of the stock as the target variable for our model.\n",
    "\n",
    "# What model(s) you used trained on your data, and how you chose them.\n",
    "\n",
    "Originally, we used rather simplistic models like logistic regression, Random Forest, and SVM in order to predict stock price movements. We utilized Recursive Feature Elimination (RFE) in order to determine the optimal features for prediction for each model. However, we found that these models were not able to predict stock price movements consistently with much accuracy. We then decided to use a Long Short-Term Memory (LSTM) model to predict stock price movements. `LSTM` models are a type of recurrent neural network (RNN) with the addition of \"gates\" notably the `input`, `forget` and `output` gates. These gates allow for the model to determine what information to retain or discard at each timestep, mitigating the vanishing descent issue found in traditional recurrent neural networks. The LSTM model accounts for the shortfalls of an RNN by capturing long-term dependencies in the data.\n",
    "\n",
    "The forget gate determines which information is either retained or discarded at each time step. It accepts the output from the previous time step $h_{t-1}$ and the input $x_t$ at the current time step. The forget gate is defined as:\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "The input gate determines which information is stored in the cell state. It avoids feeding the unimportant information into the current memory cell. It has three different components: \n",
    "\n",
    "1) Getting the state of the cell that must be updated.\n",
    "2) Create a new cell state\n",
    "3) Update the cell state to the current cell state\n",
    "\n",
    "These are defined as: \n",
    "\n",
    "\\begin{aligned}\n",
    "i_{t} &= \\sigma(W_{t} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\\n",
    "\\widetilde{C}_{t} &= \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c}) \\\\\n",
    "C_{t} &= f_{t} \\ast C_{t-1} + i_{t} \\ast \\widetilde{C}_{t}\n",
    "\\end{aligned}\n",
    "\n",
    "The output gate determines how much of the newly created cell state will be discarded and how much will be passed to the output. It is defined as:\n",
    "\n",
    "$$o_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})$$\n",
    "\n",
    "This output information is firstly determined by a sigmoid layer, then the newly created cell state is processed by a tanh layer. The output is then multiplied by the sigmoid layer to determine the final output of the LSTM cell.\n",
    "\n",
    "Which is defined as:\n",
    "\n",
    "$$h_{t} = o_{t} \\ast \\tanh(C_{t})$$\n",
    "\n",
    "Taking this all into account, the LSTM model is able to retain information from previous time steps and use it to predict future stock price movements while disregarding irrelevant information.\n",
    "\n",
    "# How you trained your models, and on what hardware.\n",
    "\n",
    "We trained our model using our own personal devices. We used the `Adam` optimizer with a learning rate of 0.001. We trained the model for 1000 epochs for each stock in our dataset (10 total). We used the `mean_squared_error` loss function to train the model.\n",
    "\n",
    "# How you evaluated your models (loss, accuracy, etc), and the size of your test set.\n",
    "\n",
    "We evaluated the accuracy of our model by comparing the predicted stock price movements to the actual stock price movements. We used the `mean_squared_error` loss function to evaluate the model. If the model predicted the next days price to be positive, we would purchase the stock at the closing price and sell it at the closing price the next day. If the model predicted the next days price to be negative, we would short the stock at the closing price and buy it back at the closing price the next day. We would then calculate the profit or loss percent change for each stock and compare it to the last value benchmark. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM on 10 Biggest Energy Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable \n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lstm_model import LSTMModel\n",
    "from lstm_data import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['XOM', 'CVX', 'COP', 'EPD', 'PXD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n",
    "start = '2014-05-06'\n",
    "end = '2024-05-07'\n",
    "period = '10y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preps data, see lstm_data.py, prints size of each ticker's dataset\n",
    "X_train, y_train, X_test, y_test, X_scalers, y_scalers, batch_size = prepare_data(tickers, start_date=start, end_date=end, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "X_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Quantitative Trading Model Using LSTM\n",
    "author: Andre Xiao, James Ohr, Donovan Wood\n",
    "date: '2024-04-22'\n",
    "image: \"quant.jpg\"\n",
    "description: \"Final Project for CS451\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = X_train_final.shape[2] # number of features\n",
    "hidden_size = 32 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "batch_size = batch_size\n",
    "window = 1 # number of windows, leave at 1, basically can ignore\n",
    "\n",
    "num_classes = 1 # number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data by ticker\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below takes about 7 min to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  for i, data in enumerate(data_loader_train):\n",
    "    X_, y_ = data\n",
    "    outputs = lstm.forward(X_) #forward pass\n",
    "    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n",
    "  \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n",
    "  \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "  \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    # if (i + 1) % 50 == 0:\n",
    "    #     print(f\"Epoch {epoch}, batch {i:>3}, loss on batch: {loss.item():.3f}\")\n",
    "  if epoch % 100 == 0:\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate_lstm(model, X_test, y_test, X_scaler, y_scaler, features):\n",
    "    ticker = X_test['Ticker'].iloc[0] # get ticker\n",
    "    X_test_tensors = Variable(torch.Tensor(np.array(X_test[features]))) # prepare for lstm\n",
    "    X_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) # prepare for lstm\n",
    "\n",
    "    test_predict = model(X_test_final).data.numpy() # predict\n",
    "    test_predict = y_scaler.inverse_transform(test_predict) # reverse transform back to original scale\n",
    "    cols = X_test.columns[X_test.columns != 'Ticker']\n",
    "    X_test = pd.DataFrame(X_scaler.inverse_transform(X_test[cols]), index=X_test.index, columns=cols) # reverse transform X_test back to og scale\n",
    "    predicted_price = pd.DataFrame(test_predict)\n",
    "    predicted_price.columns = ['Predicted_Price']\n",
    "    predicted_price.size\n",
    "    idx = X_test.index[:predicted_price.size]\n",
    "    predicted_price.index = idx # fix index of predicted prices\n",
    "    X_test = pd.concat([X_test, predicted_price], ignore_index=False, axis=1) \n",
    "    X_test = X_test.dropna()\n",
    "    X_test['Actual_Signal'] = (X_test['Returns'].shift(-1) > 0).astype(int) # actual buy/sell signal based on daily returns\n",
    "    X_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\n",
    "    X_test['Predicted_Signal'] = (X_test['Predicted_Returns'] > 0).astype(int) # predicted buy/sell signal based on predicted returns\n",
    "    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1) # calculate daily strategy returns\n",
    "    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n",
    "    returns = X_test.loc[X_test.index, 'Returns']\n",
    "    returns.iloc[0] = 0\n",
    "    cumulative_stock_returns = (returns + 1).cumprod()\n",
    "    accuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\n",
    "    print(f'{ticker} Accuracy: {accuracy}')\n",
    "\n",
    "    # plot stock price\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(X_test['Predicted_Price'].shift(1), label='Predicted Price')\n",
    "    plt.plot(X_test['Close'], label='Actual Price')\n",
    "    plt.title(f'{ticker} Price')\n",
    "    plt.legend();\n",
    "\n",
    "    # plot returns\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n",
    "    plt.plot(cumulative_stock_returns, label='Stock Returns')\n",
    "    plt.title(f'{ticker} Returns')\n",
    "    plt.legend();\n",
    "\n",
    "    # plot confusion matrix\n",
    "    cm = confusion_matrix(X_test['Actual_Signal'], X_test['Predicted_Signal'])\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=['Sell', 'Buy'])\n",
    "    cm_display.plot();\n",
    "    plt.title(f'{ticker} Confusion Matrix')\n",
    "\n",
    "    return cumulative_strategy_returns, cumulative_stock_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_strat_returns_list = np.array([])\n",
    "cum_stock_returns_list = np.array([])\n",
    "for i in range(10):\n",
    "    cum_strat_returns, cum_stock_returns = evaluate_lstm(lstm, X_test[i], y_test[i], X_scalers[i], y_scalers[i], features)\n",
    "    cum_strat_returns_list = np.append(cum_strat_returns_list, cum_strat_returns.iloc[-1])\n",
    "    cum_stock_returns_list = np.append(cum_stock_returns_list, cum_stock_returns.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_strat_returns = (cum_strat_returns_list).mean()\n",
    "total_stock_returns = (cum_stock_returns_list).mean()\n",
    "\n",
    "print(f'Portfolio Returns: {total_strat_returns}')\n",
    "print(f'Stock Returns: {total_stock_returns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Mock Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start = '2014-05-06'\n",
    "end = '2024-05-08'\n",
    "\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "X_scalers = []\n",
    "y_scalers = []\n",
    "for t in tickers:\n",
    "    ticker_data = yf.Ticker(t)\n",
    "    #data = ticker_data.history(period=period)\n",
    "    data = yf.download(t, start=start, end=end)\n",
    "\n",
    "    # Calculate moving averages and std\n",
    "    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n",
    "    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n",
    "\n",
    "    # Calculate the z-score\n",
    "    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n",
    "\n",
    "    # Calculate RSI\n",
    "    delta = data['Close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ema_up / ema_down\n",
    "\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "    # Calculate TTM EPS and P/E\n",
    "    eps = ticker_data.get_earnings_dates(limit=60)\n",
    "    eps.index = eps.index.date\n",
    "    eps = eps.loc[~eps.index.duplicated(keep='first'), :]\n",
    "    data.index = data.index.date\n",
    "    eps = eps[(eps.index >= (data.index[0]-relativedelta(years=1))) & (eps.index <= data.index[-1])]\n",
    "    if t == 'DUK':\n",
    "        eps.loc[eps.index == pd.to_datetime('2024-05-07').date(), 'Reported EPS'] = 1.44\n",
    "    eps = eps.iloc[::-1]\n",
    "    eps['TTM'] = eps['Reported EPS'].rolling(window=4).sum()\n",
    "    \n",
    "    idx = pd.date_range(eps.index[0], eps.index[-1])\n",
    "    eps = eps.reindex(idx.date, fill_value=np.nan)\n",
    "    data['TTM_EPS'] = eps['TTM'].copy()\n",
    "    data[data['TTM_EPS'].notna()]\n",
    "    data['TTM_EPS'] = data['TTM_EPS'].ffill()\n",
    "    data['TTM_EPS'] = data['TTM_EPS'].fillna(eps['TTM'].loc[eps['TTM'].notna()].iloc[0])\n",
    "    data['TTM_P/E'] = data['Close'] / data['TTM_EPS']\n",
    "\n",
    "    # Calculate the daily returns\n",
    "    data['Returns'] = data['Close'].pct_change()\n",
    "\n",
    "    # Drop any NaNs\n",
    "\n",
    "    # If stock price goes up or down\n",
    "    data['Target'] = data['Close'].shift(-1)\n",
    "\n",
    "    data = data.dropna()\n",
    "    #features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n",
    "    X = data.loc[:, data.columns != 'Target']\n",
    "    y = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n",
    "    #X = X.dropna()\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, shuffle=False)\n",
    "    \n",
    "    ss1 = StandardScaler()\n",
    "    ss2 = StandardScaler()\n",
    "\n",
    "    X_train_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns) # fit ss and transform X_train\n",
    "    y_train_ss = pd.DataFrame(ss2.fit_transform(y), index=y.index, columns=y.columns) # fit mm and transform y_train\n",
    "\n",
    "    X_train_ss['Ticker'] = t\n",
    "    print(X_train_ss.shape)\n",
    "    X_train_list.append(X_train_ss)\n",
    "    y_train_list.append(y_train_ss)\n",
    "    X_scalers.append(ss1)\n",
    "    y_scalers.append(ss2)\n",
    "batch_size = X_train_list[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat(X_train_list)\n",
    "y_train = pd.concat(y_train_list)\n",
    "\n",
    "features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "X_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = X_train_final.shape[2] # number of features\n",
    "hidden_size = 32 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "batch_size = batch_size\n",
    "window = 1 # number of windows, leave at 1, basically can ignore\n",
    "\n",
    "num_classes = 1 # number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data by ticker\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_live = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm_live.parameters(), lr=learning_rate) # ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  for i, data in enumerate(data_loader_train):\n",
    "    X_, y_ = data\n",
    "    outputs = lstm_live.forward(X_) #forward pass\n",
    "    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n",
    "  \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n",
    "  \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "  \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    # if (i + 1) % 50 == 0:\n",
    "    #     print(f\"Epoch {epoch}, batch {i:>3}, loss on batch: {loss.item():.3f}\")\n",
    "  if epoch % 100 == 0:\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(lstm_live, 'lstm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df_pred_list = []\n",
    "for j in range(7, 10):\n",
    "    tickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n",
    "    start = '2014-05-06'\n",
    "    end = f'2024-05-0{j}'\n",
    "\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    X_scalers = []\n",
    "    y_scalers = []\n",
    "    for t in tickers:\n",
    "        ticker_data = yf.Ticker(t);\n",
    "        #data = ticker_data.history(period=period)\n",
    "        data = yf.download(t, start=start, end=end);\n",
    "\n",
    "        # Calculate moving averages and std\n",
    "        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n",
    "        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n",
    "\n",
    "        # Calculate the z-score\n",
    "        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n",
    "\n",
    "        # Calculate RSI\n",
    "        delta = data['Close'].diff()\n",
    "        up = delta.clip(lower=0)\n",
    "        down = -1 * delta.clip(upper=0)\n",
    "        ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "        ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "        rs = ema_up / ema_down\n",
    "\n",
    "        data['RSI'] = 100 - (100 / (1 + rs))\n",
    "        # Calculate TTM EPS and P/E\n",
    "        eps = ticker_data.get_earnings_dates(limit=60)\n",
    "        eps.index = eps.index.date\n",
    "        eps = eps.loc[~eps.index.duplicated(keep='first'), :]\n",
    "        data.index = data.index.date\n",
    "        eps = eps[(eps.index >= (data.index[0]-relativedelta(years=1))) & (eps.index <= data.index[-1])]\n",
    "        if t == 'DUK':\n",
    "            eps.loc[eps.index == pd.to_datetime('2024-05-07').date(), 'Reported EPS'] = 1.44\n",
    "        eps = eps.iloc[::-1]\n",
    "        eps['TTM'] = eps['Reported EPS'].rolling(window=4).sum()\n",
    "        \n",
    "        \n",
    "        idx = pd.date_range(eps.index[0], eps.index[-1])\n",
    "        eps = eps.reindex(idx.date, fill_value=np.nan)\n",
    "        data['TTM_EPS'] = eps['TTM'].copy()\n",
    "        data[data['TTM_EPS'].notna()]\n",
    "        data['TTM_EPS'] = data['TTM_EPS'].ffill()\n",
    "        data['TTM_EPS'] = data['TTM_EPS'].fillna(eps['TTM'].loc[eps['TTM'].notna()].iloc[0])\n",
    "        data['TTM_P/E'] = data['Close'] / data['TTM_EPS']\n",
    "\n",
    "        #X_test_list.append(data.iloc[-1])\n",
    "\n",
    "        # Calculate the daily returns\n",
    "        data['Returns'] = data['Close'].pct_change()\n",
    "\n",
    "        # Drop any NaNs\n",
    "\n",
    "        # If stock price goes up or down\n",
    "        data['Target'] = data['Close'].shift(-1)\n",
    "\n",
    "        #data = data.dropna()\n",
    "        #features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n",
    "        X = data.loc[:, data.columns != 'Target']\n",
    "        y = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n",
    "        X = X.dropna()\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, shuffle=False)\n",
    "        \n",
    "        ss1 = StandardScaler()\n",
    "        ss2 = StandardScaler()\n",
    "\n",
    "        X_test_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns) # fit ss and transform X_train\n",
    "        y_test_ss = pd.DataFrame(ss2.fit_transform(y), index=y.index, columns=y.columns) # fit mm and transform y_train\n",
    "\n",
    "        X_test_ss['Ticker'] = t\n",
    "        print(X_train_ss.shape)\n",
    "        X_test_list.append(X_test_ss)\n",
    "        y_test_list.append(y_test_ss)\n",
    "        X_scalers.append(ss1)\n",
    "        y_scalers.append(ss2)\n",
    "    batch_size = X_test_list[0].shape[0]\n",
    "\n",
    "    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n",
    "    next_day_predict_list = []\n",
    "\n",
    "    for i, test in enumerate(X_test_list):\n",
    "        X_test_tensors = Variable(torch.Tensor(np.array(test[features])))\n",
    "        X_test_final = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n",
    "        predict = lstm_live(X_test_final).data.numpy()\n",
    "        predict = y_scalers[i].inverse_transform(predict)\n",
    "        predicted_price = pd.DataFrame(predict)\n",
    "\n",
    "        cols = test.columns[test.columns != 'Ticker']\n",
    "        test = pd.DataFrame(X_scalers[i].inverse_transform(test[cols]), index=test.index, columns=cols)\n",
    "\n",
    "        predicted_price.columns = ['Predicted_Price']\n",
    "        predicted_price.size\n",
    "        idx = test.index[:predicted_price.size]\n",
    "        predicted_price.index = idx\n",
    "\n",
    "        test = pd.concat([test, predicted_price], ignore_index=False, axis=1)\n",
    "        test['Predicted_Returns'] = test['Predicted_Price'].pct_change()\n",
    "        test['Predicted_Signal'] = (test['Predicted_Returns'] > 0)*1\n",
    "        test['Actual_Signal'] = (test['Returns'].shift(-1) > 0)*1\n",
    "        test['Ticker'] = tickers[i]\n",
    "\n",
    "        X_test_list[i] = test\n",
    "        test = test.loc[test.index == pd.to_datetime(f'2024-05-0{j-1}').date(), :]\n",
    "        next_day_predict_list.append(test[['Ticker', 'Close', 'Predicted_Price', 'Predicted_Returns', 'Predicted_Signal', 'Actual_Signal']])\n",
    "        \n",
    "    df_pred_list.append(pd.concat(next_day_predict_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_current(df_pred):    \n",
    "    current = np.array([])\n",
    "    for t in tickers:\n",
    "        data = yf.download(t, period='1d')\n",
    "        #data = data.loc[data.index == pd.to_datetime('2024-05-07'), :]\n",
    "        current = np.append(current, data['Close'])\n",
    "\n",
    "    df_pred['Actual_Price'] = current\n",
    "    df_pred['Actual_Signal'] = (df_pred['Actual_Price'] > df_pred['Close'])*1\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may7_pred = add_current(df_pred_list[0])\n",
    "may7_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(may7_pred['Predicted_Signal'] == may7_pred['Actual_Signal']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(may7_pred['Actual_Signal'], may7_pred['Predicted_Signal'])\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=['Sell', 'Buy'])\n",
    "cm_display.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may8_pred = add_current(df_pred_list[1])\n",
    "may8_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(may8_pred['Predicted_Signal'] == may8_pred['Actual_Signal']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(may8_pred['Actual_Signal'], may8_pred['Predicted_Signal'])\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=['Sell', 'Buy'])\n",
    "cm_display.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
